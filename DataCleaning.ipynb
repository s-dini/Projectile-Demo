{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0625c2d7-527a-4c9e-807d-c934f9f606c4",
   "metadata": {},
   "source": [
    "## Example: Dropping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "071e5ab0-32ac-4579-80e4-e31fd21be22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of missing values in each column\n",
      "--------------------------------\n",
      "country      0\n",
      "1990         3\n",
      "1991       157\n",
      "1992       146\n",
      "1993       132\n",
      "1994       111\n",
      "1995        66\n",
      "1996        34\n",
      "1997        24\n",
      "1998        18\n",
      "1999        12\n",
      "2000        10\n",
      "2001         8\n",
      "2002         7\n",
      "2003        13\n",
      "2004        10\n",
      "2005         9\n",
      "2006        10\n",
      "2007         3\n",
      "2008         4\n",
      "2009         5\n",
      "2010         5\n",
      "2011         2\n",
      "2012         5\n",
      "2013         4\n",
      "2014         4\n",
      "2015         4\n",
      "2016         6\n",
      "2017         8\n",
      "2018       111\n",
      "2019       125\n",
      "dtype: int64\n",
      "\n",
      "Sample Rows with Missing Values\n",
      "------------------------\n",
      "       country    2001  2002  2003  2004\n",
      "8    Australia  52.700   NaN   NaN   NaN\n",
      "9      Austria  39.200  36.6  42.7  54.3\n",
      "10  Azerbaijan   0.306   5.0   NaN   NaN\n",
      "11     Bahamas  11.800  18.0  20.0  22.0\n",
      "\n",
      "Sample Rows with Filled in Values\n",
      "------------------------\n",
      "       country   2001  2002  2003  2004\n",
      "8    Australia   52.7  52.7  52.7  52.7\n",
      "9      Austria   39.2  36.6  42.7  54.3\n",
      "10  Azerbaijan  0.306   5.0   5.0   5.0\n",
      "11     Bahamas   11.8  18.0  20.0  22.0\n",
      "\n",
      "Number of Duplicate Rows\n",
      "------------------------\n",
      "1\n",
      "\n",
      "Duplicate Row\n",
      "----------------\n",
      "88    Kenya\n",
      "Name: country, dtype: object\n",
      "\n",
      "\n",
      "Number of Duplicate Rows Now\n",
      "------------------------\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\200516665\\AppData\\Local\\Temp\\ipykernel_23656\\3904860338.py:22: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method='ffill', axis=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv (\"countries.csv\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# drop unnecessary columns\n",
    "df = df.drop([\"1985\", \"1986\",\"1987\",\"1988\", \"1989\"], axis=1)\n",
    "\n",
    "# determine the number of missing values\n",
    "print(\"List of missing values in each column\")\n",
    "print(\"--------------------------------\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# since each column is the next year, it makes the most sense\n",
    "# to fill all missing values with the number/percentage \n",
    "# that comes before it in the previous column (previous year)\n",
    "print()\n",
    "print(\"Sample Rows with Missing Values\")\n",
    "print(\"------------------------\")\n",
    "print(df.loc[8:11, [\"country\", \"2001\", \"2002\", \"2003\", \"2004\"]])\n",
    "df = df.fillna(method='ffill', axis=1)\n",
    "print()\n",
    "print(\"Sample Rows with Filled in Values\")\n",
    "print(\"------------------------\")\n",
    "print(df.loc[8:11, [\"country\", \"2001\", \"2002\", \"2003\", \"2004\"]])\n",
    "\n",
    "# determine the number of duplicate rows\n",
    "print()\n",
    "print(\"Number of Duplicate Rows\")\n",
    "print(\"------------------------\")\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "\n",
    "# find the duplicated row(s)!\n",
    "print()\n",
    "print(\"Duplicate Row\")\n",
    "print(\"----------------\")\n",
    "print(df.loc[df.duplicated(), \"country\"])\n",
    "\n",
    "\n",
    "# drop all duplicate rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "print()\n",
    "print()\n",
    "print(\"Number of Duplicate Rows Now\")\n",
    "print(\"------------------------\")\n",
    "print(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b6c7f-b296-407d-bb8c-8de89434aba9",
   "metadata": {},
   "source": [
    "## Example: Fixing Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "685adae0-37aa-47eb-8bf4-6866a80cfde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataTypes\n",
      "-----------\n",
      "number           int64\n",
      "name            object\n",
      "position        object\n",
      "games_played     int64\n",
      "goals            int64\n",
      "assists          int64\n",
      "birthdate       object\n",
      "birthplace      object\n",
      "height          object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "DataTypes Now\n",
      "-----------\n",
      "number           object\n",
      "name             object\n",
      "position         object\n",
      "games_played      int64\n",
      "goals             int64\n",
      "assists           int64\n",
      "birthdate        object\n",
      "birthplace       object\n",
      "height          float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv (\"soccer.csv\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# add a height column\n",
    "df[\"height\"] = [\"5.08\", \"6.01\", \"5.03\", \"5.04\", \"5.07\",\n",
    "                \"5.04\", \"5.03\", \"5.07\", \"\", \"5.11\", \"5.08\",\n",
    "                \"5.05\", \"5.06\", \"5.09\", \"5.09\", \"5.07\",\n",
    "                \"5.09\", \"5.08\", \"5.1\", \"5.07\", \"5.11\"]\n",
    "                \n",
    "# check the data types of all columns\n",
    "print(\"DataTypes\")\n",
    "print(\"-----------\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# change the data type of the number column to a string\n",
    "# df.number = df.number.astype(str)\n",
    "\n",
    "# change the data type of the height column to a float \n",
    "df.height = pd.to_numeric(df.height)\n",
    "print()\n",
    "print()\n",
    "print(\"DataTypes Now\")\n",
    "print(\"-----------\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# What issues might you still run into with using the height column?\n",
    "\n",
    "# could be zero or null "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945968cb-5209-4d28-8705-9cf0ea20eeb3",
   "metadata": {},
   "source": [
    "## Assignment: Cleaning Book Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc2e083d-062f-42ee-85ec-db2c464bb1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title              object\n",
      "author             object\n",
      "rating            float64\n",
      "voters            float64\n",
      "price             float64\n",
      "publisher          object\n",
      "page_count        float64\n",
      "published_date     object\n",
      "dtype: object\n",
      "Duplicated Rows\n",
      "-----------\n",
      "1052\n",
      "List of missing values in each column\n",
      "--------------------------------\n",
      "title          0\n",
      "author         0\n",
      "rating        14\n",
      "voters        14\n",
      "price          0\n",
      "page_count     0\n",
      "dtype: int64\n",
      "List of missing values in each column [NEW]\n",
      "--------------------------------\n",
      "title         0\n",
      "author        0\n",
      "rating        0\n",
      "voters        0\n",
      "price         0\n",
      "page_count    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# <-- Click on the books.csv file to see the dataset.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# import the data from the books.csv file\n",
    "\n",
    "df = pd.read_csv(\"books.csv\")\n",
    "\n",
    "# set max_columns to None\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Check the data types. Do they look okay?\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "# Drop the publisher and published_date columns.\n",
    "\n",
    "df = df.drop([\"publisher\", \"published_date\"], axis=1)\n",
    "\n",
    "# Print the shape of the data and check for duplicate rows. \n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "\n",
    "# How many are there?\n",
    "\n",
    "print(\"Duplicated Rows\")\n",
    "print(\"-----------\")\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "\n",
    "# Drop duplicate rows. \n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Determine the number of missing values.\n",
    "\n",
    "print(\"List of missing values in each column\")\n",
    "print(\"--------------------------------\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# What would be the best decision for dealing with\n",
    "# the missing values? Make the call and change\n",
    "# the data.\n",
    "\n",
    "# because all books that were missing an entry were missing both voters and rating we decided to drop the book\n",
    "df = df.dropna()\n",
    "\n",
    "print(\"List of missing values in each column [NEW]\")\n",
    "print(\"--------------------------------\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a29bd3-cff1-4f28-9fba-9185e9b5579b",
   "metadata": {},
   "source": [
    "## Assignment: Find some data on the sites recommended in the textbook materials. Clean the data, train a machine learning models and test it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d494423b-50c9-4fdc-95a8-702eaafb46c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name                                           object\n",
      "English Name                                   object\n",
      "Image source                                   object\n",
      "Synopsis                                       object\n",
      "Rating                                        float64\n",
      "Rated by(number of users)                      object\n",
      "Rank                                          float64\n",
      "Popularity                                      int64\n",
      "Release time                                   object\n",
      "Number of episodes                            float64\n",
      "Duration                                       object\n",
      "Status                                         object\n",
      "Aired                                          object\n",
      "Producers                                      object\n",
      "Studio(s)                                      object\n",
      "Genres                                         object\n",
      "Theme                                          object\n",
      "Demographic                                    object\n",
      "Anime recomendations(by users and autorec)     object\n",
      "dtype: object\n",
      "\n",
      "                                                Name            English Name  \\\n",
      "0                                       !NVADE SHOW!                     NaN   \n",
      "1                                                \"0\"                     NaN   \n",
      "2  \"Aesop\" no Ohanashi yori: Ushi to Kaeru, Yokub...                     NaN   \n",
      "3                                     \"Ai\" wo Taberu  Eating \"Love & Sorrow\"   \n",
      "4          \"Bungaku Shoujo\" Kyou no Oyatsu: Hatsukoi                     NaN   \n",
      "\n",
      "                                        Image source  \\\n",
      "0  https://cdn.myanimelist.net/images/anime/1930/...   \n",
      "1  https://cdn.myanimelist.net/images/anime/12/81...   \n",
      "2  https://cdn.myanimelist.net/images/anime/3/651...   \n",
      "3  https://cdn.myanimelist.net/images/anime/1948/...   \n",
      "4  https://cdn.myanimelist.net/images/anime/1671/...   \n",
      "\n",
      "                                            Synopsis  Rating  \\\n",
      "0  Animated music video for RAISE A SUILEN's song...    5.88   \n",
      "1  This music video tells how a shy girl with a s...    4.71   \n",
      "2                           Based on Aesop's Fables.    5.59   \n",
      "3  Graduate Work 2018 by Yuuta Watanabe, Tokyo Po...     NaN   \n",
      "4  Short episode bundled with the limited edition...    6.84   \n",
      "\n",
      "  Rated by(number of users)     Rank  Popularity Release time  \\\n",
      "0                       492      NaN       14690          NaN   \n",
      "1                     2,419      NaN        9391          NaN   \n",
      "2                       131  11722.0       16064          NaN   \n",
      "3                       NaN  18814.0       22881          NaN   \n",
      "4                    10,784   5061.0        5077          NaN   \n",
      "\n",
      "   Number of episodes Duration           Status         Aired  \\\n",
      "0                 1.0   2 min.  Finished Airing   Oct 7, 2020   \n",
      "1                 1.0   4 min.  Finished Airing  Sep 11, 2013   \n",
      "2                 1.0  12 min.  Finished Airing  Mar 21, 1970   \n",
      "3                 1.0   8 min.  Finished Airing          2018   \n",
      "4                 1.0  15 min.  Finished Airing  Dec 26, 2009   \n",
      "\n",
      "                                           Producers                Studio(s)  \\\n",
      "0                                                NaN                      NaN   \n",
      "1                       ['Sony Music Entertainment']  ['Minakata Laboratory']   \n",
      "2                                                NaN                      NaN   \n",
      "3                   ['Tokyo Polytechnic University']                      NaN   \n",
      "4  ['Lantis', 'Pony Canyon', 'Enterbrain', 'Kadok...       ['Production I.G']   \n",
      "\n",
      "                            Genres   Theme Demographic  \\\n",
      "0                        ['Music']   Music         NaN   \n",
      "1                        ['Music']   Music         NaN   \n",
      "2                               []     NaN        Kids   \n",
      "3                        ['Drama']     NaN         NaN   \n",
      "4  ['Comedy', 'Fantasy', 'School']  School         NaN   \n",
      "\n",
      "          Anime recomendations(by users and autorec)  \n",
      "0  [{'title': 'Hekiten Bansou', 'users': 'AutoRec...  \n",
      "1  [{'title': '24H', 'users': 'AutoRec'}, {'title...  \n",
      "2  [{'title': '0-sen Hayato', 'users': 'AutoRec'}...  \n",
      "3  [{'title': 'Eizouken ni wa Te wo Dasu na!', 'u...  \n",
      "4  [{'title': 'Ookami to Koushinryou', 'users': '...  \n",
      "\n",
      "   Rating     Rank  Popularity\n",
      "0    5.88      NaN       14690\n",
      "1    4.71      NaN        9391\n",
      "2    5.59  11722.0       16064\n",
      "3     NaN  18814.0       22881\n",
      "4    6.84   5061.0        5077\n",
      "Duplicated Rows\n",
      "-----------\n",
      "277\n",
      "   Rating     Rank  Popularity\n",
      "0    5.88      NaN       14690\n",
      "1    4.71      NaN        9391\n",
      "2    5.59  11722.0       16064\n",
      "3     NaN  18814.0       22881\n",
      "4    6.84   5061.0        5077\n",
      "\n",
      "List of missing values in each column\n",
      "--------------------------------\n",
      "Rating        9594\n",
      "Rank          6207\n",
      "Popularity       0\n",
      "dtype: int64\n",
      "Mean Absolute Error:  12.050852843955871\n",
      "Mean Squared Error:  250.60424244992203\n",
      "R² Score:  0.9999839193083815\n",
      "Accuracy:  0.3663003663003663\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# import the data from the anime.csv file\n",
    "\n",
    "df = pd.read_csv(\"anime_dataset.csv\")\n",
    "\n",
    "# set max_columns to None\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Check the data types. Do they look okay?\n",
    "\n",
    "print(df.dtypes)\n",
    "print()\n",
    "print(df.head())\n",
    "print() \n",
    "\n",
    "# Drop columns\n",
    "\n",
    "df = df.drop([\"Name\", \"English Name\", \"Image source\", \"Synopsis\", \"Rated by(number of users)\", \"Release time\", \"Number of episodes\", \"Duration\", \n",
    "              \"Status\", \"Aired\", \"Producers\", \"Studio(s)\", \"Genres\", \"Theme\", \"Demographic\", \"Anime recomendations(by users and autorec)\" ], axis=1)\n",
    "\n",
    "# Print the shape of the data and check for duplicate rows. \n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# How many are there?\n",
    "\n",
    "print(\"Duplicated Rows\")\n",
    "print(\"-----------\")\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "# Drop duplicate rows. \n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# Determine the number of missing values.\n",
    "\n",
    "print()\n",
    "print(\"List of missing values in each column\")\n",
    "print(\"--------------------------------\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# What would be the best decision for dealing with\n",
    "# the missing values? Make the call and change\n",
    "# the data.\n",
    "\n",
    "# we delete the row! it is too complex to fill it in \n",
    "# (though it could be done) and deleting those rows \n",
    "# have a negligible difference (since there are thousands of rows) \n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_and_val_data = train_test_split(df, test_size = .20, random_state = 2) #split into train and an aggragated test and val\n",
    "val_data, test_data = train_test_split(test_and_val_data, test_size = .50, random_state = 2) #split aggregagted into test and val\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "Y = train_data['Rank']\n",
    "X = train_data[['Rating','Popularity']]\n",
    "model = RandomForestRegressor(n_estimators=10, max_depth=15)\n",
    "\n",
    "model.fit(X,Y)\n",
    "\n",
    "val_Y = val_data['Rank']\n",
    "val_X = val_data[['Rating','Popularity']]\n",
    "predictions = model.predict(val_X)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "mae = mean_absolute_error(val_Y, predictions)\n",
    "mse = mean_squared_error(val_Y, predictions)\n",
    "r2 = r2_score(val_Y, predictions)\n",
    "\n",
    "print(\"Mean Absolute Error: \", mae)\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"R² Score: \", r2)\n",
    "\n",
    "accuracy = np.sum(val_Y == predictions)/len(val_X)*100\n",
    "print(\"Accuracy: \" , accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8ed0f27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        1\n",
      "1        0\n",
      "2        0\n",
      "3        1\n",
      "4        0\n",
      "        ..\n",
      "27896    0\n",
      "27897    0\n",
      "27898    0\n",
      "27899    1\n",
      "27900    1\n",
      "Name: Depression, Length: 27901, dtype: int64\n",
      "id                                         int64\n",
      "Gender                                    object\n",
      "Age                                      float64\n",
      "City                                      object\n",
      "Profession                                object\n",
      "Academic Pressure                        float64\n",
      "Work Pressure                            float64\n",
      "CGPA                                     float64\n",
      "Study Satisfaction                       float64\n",
      "Job Satisfaction                         float64\n",
      "Sleep Duration                            object\n",
      "Dietary Habits                            object\n",
      "Degree                                    object\n",
      "Have you ever had suicidal thoughts ?     object\n",
      "Work/Study Hours                         float64\n",
      "Financial Stress                         float64\n",
      "Family History of Mental Illness          object\n",
      "Depression                                 int64\n",
      "dtype: object\n",
      "[]\n",
      "Empty DataFrame\n",
      "Columns: [Age, Have you ever had suicidal thoughts ?, Depression]\n",
      "Index: []\n",
      "\n",
      "Duplicated Rows\n",
      "-----------\n",
      "0\n",
      "List of missing values in each column\n",
      "--------------------------------\n",
      "Age                                      0\n",
      "Have you ever had suicidal thoughts ?    0\n",
      "Depression                               0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 78\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# What would be the best decision for dealing with\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# the missing values? Make the call and change\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# the data.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# we do not have any missing values. yay!\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m---> 78\u001b[0m train_data, test_and_val_data \u001b[38;5;241m=\u001b[39m train_test_split(df, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.20\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m#split into train and an aggragated test and val\u001b[39;00m\n\u001b[0;32m     79\u001b[0m val_data, test_data \u001b[38;5;241m=\u001b[39m train_test_split(test_and_val_data, test_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.50\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m#split aggregagted into test and val\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n",
      "File \u001b[1;32mc:\\Users\\200516665\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\200516665\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2660\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2661\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2662\u001b[0m )\n\u001b[0;32m   2664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\200516665\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2308\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2305\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2312\u001b[0m     )\n\u001b[0;32m   2314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "'''\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# import the data from the file\n",
    "\n",
    "df = pd.read_csv(\"Student Depression Dataset.csv\")\n",
    "\n",
    "# set max_columns to None\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Check the data types. Do they look okay?\n",
    "print(df['Depression'])\n",
    "\n",
    "# format the data\n",
    "\n",
    "df['Gender'] = np.where(df['Gender'] == 'Male', 0, df['Gender'])\n",
    "df['Gender'] = np.where(df['Gender'] == 'Female', 1, df['Gender'])\n",
    "\n",
    "\n",
    "df['Have you ever had suicidal thoughts ?'] = np.where(df['Have you ever had suicidal thoughts ?'] == 'Yes', 1, df['Have you ever had suicidal thoughts ?'])\n",
    "df['Have you ever had suicidal thoughts ?'] = np.where(df['Have you ever had suicidal thoughts ?'] == 'No', 0, df['Have you ever had suicidal thoughts ?'])\n",
    "\n",
    "df['Dietary Habits'] = np.where(df['Dietary Habits'] == 'Healthy', 0, df['Dietary Habits'])\n",
    "df['Dietary Habits'] = np.where(df['Dietary Habits'] == 'Moderate', 1, df['Dietary Habits'])\n",
    "df['Dietary Habits'] = np.where(df['Dietary Habits'] == 'Unhealthy', 2, df['Dietary Habits'])\n",
    "\n",
    "print((df.dtypes))\n",
    "df = df[df['Dietary Habits'] == \"Others\"]\n",
    "df = df[df['Sleep Duration'] == \"Others\"]\n",
    "\n",
    "print(df['Dietary Habits'].unique())\n",
    "\n",
    "\n",
    "'''\n",
    "df['Sleep Duration'] = np.where(df['Sleep Duration'] == '5-6 hours', 5.5, df['Sleep Duration'])\n",
    "df['Sleep Duration'] = np.where(df['Sleep Duration'] == '7-8 hours', 7.5, df['Sleep Duration'])\n",
    "df['Sleep Duration'] = np.where(df['Sleep Duration'] == 'Less than 5 hours', 2.5, df['Sleep Duration'])\n",
    "df['Sleep Duration'] = np.where(df['Sleep Duration'] == 'More than 8 hours', 9, df['Sleep Duration'])\n",
    "'''\n",
    "\n",
    "# Drop columns\n",
    "\n",
    "df = df.drop([\"id\",\"Gender\", \"City\",\"Profession\",\"Academic Pressure\",\"Work Pressure\",\"CGPA\",\"Study Satisfaction\", \n",
    "              \"Job Satisfaction\", \"Sleep Duration\", \"Dietary Habits\",\"Degree\", \n",
    "               \"Work/Study Hours\", \"Financial Stress\", \"Family History of Mental Illness\" ], axis=1)\n",
    "\n",
    "# Print the shape of the data and check for duplicate rows. \n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# How many are there?\n",
    "\n",
    "print()\n",
    "print(\"Duplicated Rows\")\n",
    "print(\"-----------\")\n",
    "print(df.duplicated().sum())\n",
    "\n",
    "\n",
    "# Drop duplicate rows. \n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Determine the number of missing values.\n",
    "\n",
    "print(\"List of missing values in each column\")\n",
    "print(\"--------------------------------\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "\n",
    "# What would be the best decision for dealing with\n",
    "# the missing values? Make the call and change\n",
    "# the data.\n",
    "\n",
    "# we do not have any missing values. yay!\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_and_val_data = train_test_split(df, test_size = .20, random_state = 2) #split into train and an aggragated test and val\n",
    "val_data, test_data = train_test_split(test_and_val_data, test_size = .50, random_state = 2) #split aggregagted into test and val\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Y = train_data['Depression'].values.ravel()  # Ensure target is 1D\n",
    "X = train_data[['Have you ever had suicidal thoughts ?']]\n",
    "\n",
    "model = RandomForestClassifier(100, max_depth = 20)\n",
    "\n",
    "model.fit(X,Y)\n",
    "\n",
    "val_Y = val_data[['Depression']].values.ravel()\n",
    "val_X = val_data[['Have you ever had suicidal thoughts ?']]\n",
    "predictions = model.predict(val_X)\n",
    "\n",
    "accuracy = np.sum(val_Y == predictions)/len(val_X)*100\n",
    "print(\"Accuracy: \" , accuracy)\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
